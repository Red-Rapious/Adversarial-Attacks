{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Implementation of the \"NULL labeling\" defence mechanism\n",
    "This notebook implements the NULL labeling defence mechanism, protecting a model from adversarial attacks by allowing it to classify attacked images as NULL. Experiments are conducted on a simple CNN model trained on the MNIST dataset, for attacks done using the Fast Gradient Sign Method (FGSM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training the classifier on clean images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "\n",
    "#We take the MNIST dataset\n",
    "\n",
    "mean, std = 0.1307, 0.3081 #we need those for fgsm\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) \n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform) #60k images\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform) #10k images\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def new_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 1x28x28 -> 32x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 32x28x28 -> 64x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # 64x28x28 -> 64x14x14\n",
    "        nn.Flatten(),  # 64x14x14 -> 12544\n",
    "        nn.Linear(64 * 14 * 14, 128),  # 12544 -> 128\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)  # 128 -> 10\n",
    "    )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def test_model(model, test_loader, criterion, optimizer):\n",
    "    model.eval()\n",
    "    correct, total, test_loss = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs.requires_grad = True\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "        print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.1285\n",
      "Epoch 2/5, Loss: 0.0422\n",
      "Epoch 3/5, Loss: 0.0293\n",
      "Epoch 4/5, Loss: 0.0223\n",
      "Epoch 5/5, Loss: 0.0183\n",
      "Test Loss: 0.0377\n",
      "Accuracy: 98.85%\n"
     ]
    }
   ],
   "source": [
    "model_reg = new_model()\n",
    "optimizer_reg = optim.Adam(model_reg.parameters(), lr=0.001, weight_decay=1e-4) #with L2 regularization\n",
    "train_model(model_reg, train_loader, criterion, optimizer_reg, epochs=5)\n",
    "test_model(model_reg, test_loader, criterion, optimizer_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing adversarial examples and `NULL` probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model_null():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 1x28x28 -> 32x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 32x28x28 -> 64x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # 64x28x28 -> 64x14x14\n",
    "        nn.Flatten(),  # 64x14x14 -> 12544\n",
    "        nn.Linear(64 * 14 * 14, 128),  # 12544 -> 128\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 11)  # 128 -> 11, 10 +  null\n",
    "    )\n",
    "\n",
    "#We dynamically define images that are adversarial to the current model using fgsm, and train it to work on them\n",
    "def train_model_null(model, train_loader, criterion, optimizer, epochs=5, epsilon=0.5, alpha=1/11):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss, running_loss_null = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs.requires_grad = True\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            inputs_null = (torch.clamp(mean + std*(inputs + epsilon * torch.sign(inputs.grad)), 0, 1) - mean)/ std\n",
    "            outputs_null = model(inputs_null)\n",
    "            loss_null = criterion(outputs_null, torch.zeros(len(inputs), dtype=torch.int64)+10)\n",
    "            (alpha*loss_null).backward() #keep classes balanced with alpha\n",
    "            optimizer.step()\n",
    "            running_loss_null += loss_null.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}, Null Loss: {running_loss_null / len(train_loader):.4f}\")\n",
    "        \n",
    "    \n",
    "def test_model_null(model, test_loader, criterion, optimizer, epsilon=0.5):\n",
    "    model.eval()\n",
    "    correct, total, test_loss = 0, 0, 0\n",
    "    false_nulls, correct_null, test_loss_null = 0, 0, 0\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs.requires_grad = True\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        false_nulls += (predicted != 10).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        inputs_null = (torch.clamp(mean + std*(inputs + epsilon * torch.sign(inputs.grad)), 0, 1) - mean)/ std\n",
    "        outputs_null = model(inputs_null)\n",
    "        loss_null = criterion(outputs_null, torch.zeros(len(inputs), dtype=torch.int64)+10)\n",
    "        test_loss_null += loss_null.item()\n",
    "\n",
    "        _, predicted_null = torch.max(outputs_null, 1)\n",
    "        correct_null += (predicted_null == 10).sum().item()\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "    print(f\"Null Test Loss: {test_loss_null / len(test_loader):.5f}\")\n",
    "    print(f\"Null Accuracy: on null data: {100 * correct_null / total:.5f}% - on non-null data: {100 * false_nulls / total:.5f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0474\n",
      "Accuracy: 98.71%\n",
      "Null Test Loss: 0.00000\n",
      "Null Accuracy: on null data: 100.00000% - on non-null data: 99.99000\n"
     ]
    }
   ],
   "source": [
    "model_null = new_model_null()\n",
    "optimizer_null = optim.Adam(model_null.parameters(), lr=0.001)\n",
    "train_model_null(model_null, train_loader, criterion, optimizer_null, epochs=5)\n",
    "test_model_null(model_null, test_loader, criterion, optimizer_null)\n",
    "\n",
    "# good fgsm, epsilon=0.5\n",
    "# Epoch 1/5, Loss: 0.1608, Null Loss: 0.0128\n",
    "# Epoch 2/5, Loss: 0.0433, Null Loss: 0.0001\n",
    "# Epoch 3/5, Loss: 0.0275, Null Loss: 0.0001\n",
    "# Epoch 4/5, Loss: 0.0185, Null Loss: 0.0001\n",
    "# Epoch 5/5, Loss: 0.0141, Null Loss: 0.0006\n",
    "# Test Loss: 0.0481\n",
    "# Accuracy: 98.80%\n",
    "# Null Test Loss: 0.00000\n",
    "# Null Accuracy: on null data: 100.00000% - on non-null data: 99.99000%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.1806, Null Loss: 1.7469\n",
      "Epoch 2/5, Loss: 0.0439, Null Loss: 0.2885\n",
      "Epoch 3/5, Loss: 0.0295, Null Loss: 0.1984\n",
      "Epoch 4/5, Loss: 0.0321, Null Loss: 0.3131\n",
      "Epoch 5/5, Loss: 0.0181, Null Loss: 0.1452\n",
      "\n",
      "epsilon = 3\n",
      "Test Loss: 0.0338\n",
      "Accuracy: 98.97%\n",
      "Null Test Loss: 0.00625\n",
      "Null Accuracy: on null data: 99.94000% - on non-null data: 100.00000%\n",
      "\n",
      "epsilon = 2\n",
      "Test Loss: 0.0338\n",
      "Accuracy: 98.97%\n",
      "Null Test Loss: 0.00000\n",
      "Null Accuracy: on null data: 100.00000% - on non-null data: 100.00000%\n",
      "\n",
      "epsilon = 1\n",
      "Test Loss: 0.0338\n",
      "Accuracy: 98.97%\n",
      "Null Test Loss: 0.00000\n",
      "Null Accuracy: on null data: 100.00000% - on non-null data: 100.00000%\n",
      "\n",
      "epsilon = 0.5\n",
      "Test Loss: 0.0338\n",
      "Accuracy: 98.97%\n",
      "Null Test Loss: 0.00000\n",
      "Null Accuracy: on null data: 100.00000% - on non-null data: 100.00000%\n",
      "\n",
      "epsilon = 0.1\n",
      "Test Loss: 0.0338\n",
      "Accuracy: 98.97%\n",
      "Null Test Loss: 0.00000\n",
      "Null Accuracy: on null data: 100.00000% - on non-null data: 100.00000%\n",
      "\n",
      "epsilon = 0.01\n",
      "Test Loss: 0.0338\n",
      "Accuracy: 98.97%\n",
      "Null Test Loss: 0.20607\n",
      "Null Accuracy: on null data: 94.21000% - on non-null data: 100.00000%\n",
      "\n",
      "epsilon = 0.001\n",
      "Test Loss: 0.0338\n",
      "Accuracy: 98.97%\n",
      "Null Test Loss: 7.98443\n",
      "Null Accuracy: on null data: 0.01000% - on non-null data: 100.00000%\n"
     ]
    }
   ],
   "source": [
    "model_null_reg = new_model_null()\n",
    "optimizer_null_reg = optim.Adam(model_null_reg.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "train_model_null(model_null_reg, train_loader, criterion, optimizer_null_reg, epochs=5, epsilon=0.01)\n",
    "for epsilon in [3, 2, 1, 0.5, 0.1, 0.01, 0.001]:\n",
    "    print(f\"\\nepsilon = {epsilon}\")\n",
    "    test_model_null(model_null_reg, test_loader, criterion, optimizer_null_reg, epsilon=epsilon) \n",
    "\n",
    "# training on epsilon=0.5\n",
    "# Epoch 1/5, Loss: 0.1191, Null Loss: 0.0231\n",
    "# Epoch 2/5, Loss: 0.0402, Null Loss: 0.0012\n",
    "# Epoch 3/5, Loss: 0.0282, Null Loss: 0.0006\n",
    "# Epoch 4/5, Loss: 0.0204, Null Loss: 0.0008\n",
    "# Epoch 5/5, Loss: 0.0181, Null Loss: 0.0008\n",
    "# Test Loss: 0.0415\n",
    "# Accuracy: 98.83%\n",
    "# Null Test Loss: 0.00000\n",
    "# Null Accuracy: on null data: 99.99000% - on non-null data: 100.00000%\n",
    "# for epsilon=0.01\n",
    "# Null Test Loss: 17.07276\n",
    "# Null Accuracy: 0.00000%\n",
    "# for epsilon=0.1\n",
    "# Null Test Loss: 10.23794\n",
    "# Null Accuracy: 0.21000% \n",
    "# for epsilon=1\n",
    "# Null Test Loss: 0.00000\n",
    "# Null Accuracy: 100.00000%\n",
    "\n",
    "# training with epsilon=0.01\n",
    "# Epoch 1/5, Loss: 0.1806, Null Loss: 1.7469\n",
    "# Epoch 2/5, Loss: 0.0439, Null Loss: 0.2885\n",
    "# Epoch 3/5, Loss: 0.0295, Null Loss: 0.1984\n",
    "# Epoch 4/5, Loss: 0.0321, Null Loss: 0.3131\n",
    "# Epoch 5/5, Loss: 0.0181, Null Loss: 0.1452\n",
    "# Test Loss: 0.0338\n",
    "# Accuracy: 98.97% - non-null classification on non-null data: 100.00000%\n",
    "# epsilon = 3\n",
    "# Null Test Loss: 0.00625\n",
    "# Null Accuracy: 99.94000% \n",
    "# epsilon = 2\n",
    "# Null Test Loss: 0.00000\n",
    "# Null Accuracy: 100.00000%\n",
    "# epsilon = 1\n",
    "# Null Test Loss: 0.00000\n",
    "# Null Accuracy: 100.00000%\n",
    "# epsilon = 0.5\n",
    "# Null Test Loss: 0.00000\n",
    "# Null Accuracy: 100.00000%\n",
    "# epsilon = 0.1\n",
    "# Null Test Loss: 0.00000\n",
    "# Null Accuracy: 100.00000%\n",
    "# epsilon = 0.01\n",
    "# Null Test Loss: 0.20607\n",
    "# Null Accuracy: 94.21000%\n",
    "# epsilon = 0.001\n",
    "# Null Test Loss: 7.98443\n",
    "# Null Accuracy: 0.01000%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that models trained to also find whether an image is adversarial work very well. They don't suffer much loss in accuracy at all, and they're also able to very reliably identify adversarial images. However, we find that null classification only really works against images that are altered at least as much as the training data. Therefore there are potential trade-offs here, between trading for sufficiently low epsilon to train robustness for enough attacks, and epsilon high enough to have well distinguished classes. This tradeoff is not actually damaging anyway since the attacks we fail to recognize are also too weak to actually influence the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
