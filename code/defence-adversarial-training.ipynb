{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Implementation of \"adversarial training\" as a defence mechanism\n",
    "The goal of this notebook is to showcase a simple implementation of adversarial training as a defence mechanism against adversarial attacks. We compare the performance of a model trained with and without adversarial training against adversarial attacks. We use the MNIST as a dataset, a simple CNN as the model, and the Fast Gradient Sign Method (FGSM) as the adversarial attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training on augmented dataset\n",
    "### 1.1 Training on regular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We take the MNIST dataset\n",
    "\n",
    "mean, std = 0.1307, 0.3081\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean,), (std,)) \n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform) #60k images\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform) #10k images\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 1x28x28 -> 32x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 32x28x28 -> 64x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # 64x28x28 -> 64x14x14\n",
    "        nn.Flatten(),  # 64x14x14 -> 12544\n",
    "        nn.Linear(64 * 14 * 14, 128),  # 12544 -> 128\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)  # 128 -> 10\n",
    "    )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def test_model(model, test_loader, criterion, optimizer, epsilon=0.5):\n",
    "    model.eval()\n",
    "    correct, total, test_loss = 0, 0, 0\n",
    "    correct_aug, test_loss_aug = 0, 0\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs.requires_grad = True\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        inputs_aug = (torch.clamp(mean + std*(inputs + epsilon * torch.sign(inputs.grad)), 0, 1) - mean)/ std\n",
    "        outputs_aug = model(inputs_aug)\n",
    "        loss_aug = criterion(outputs_aug, labels)\n",
    "        test_loss_aug += loss_aug.item()\n",
    "\n",
    "        _, predicted_aug = torch.max(outputs_aug, 1)\n",
    "        correct_aug += (predicted_aug == labels).sum().item()\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "    print(f\"Adversarial Test Loss: {test_loss_aug / len(test_loader):.4f}\")\n",
    "    print(f\"Adversarial Accuracy: {100 * correct_aug / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0392\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 6.6889\n",
      "Adversarial Accuracy: 27.06%\n"
     ]
    }
   ],
   "source": [
    "model = new_model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5) #2 epochs decent, 10 would overfit, 5 is good\n",
    "test_model(model, test_loader, criterion, optimizer, epsilon=2)\n",
    "\n",
    "# epsilon=2\n",
    "# Adversarial Test Loss: 6.8014\n",
    "# Adversarial Accuracy: 15.87%\n",
    "\n",
    "#epsilon = 1:\n",
    "# Adversarial Test Loss: 5.0315\n",
    "# Adversarial Accuracy: 21.07%\n",
    "\n",
    "# epsilon=0.5\n",
    "# Epoch 1/5, Loss: 0.1216\n",
    "# Epoch 2/5, Loss: 0.0369\n",
    "# Epoch 3/5, Loss: 0.0226\n",
    "# Epoch 4/5, Loss: 0.0148\n",
    "# Epoch 5/5, Loss: 0.0104\n",
    "# Test Loss: 0.0332\n",
    "# Accuracy: 99.03%\n",
    "# Adversarial Test Loss: 1.4371\n",
    "# Adversarial Accuracy: 69.06%\n",
    "\n",
    "# epsilon=0.1\n",
    "# Epoch 1/5, Loss: 0.1157\n",
    "# Epoch 2/5, Loss: 0.0367\n",
    "# Epoch 3/5, Loss: 0.0224\n",
    "# Epoch 4/5, Loss: 0.0151\n",
    "# Epoch 5/5, Loss: 0.0120\n",
    "# Test Loss: 0.0392\n",
    "# Accuracy: 98.90%\n",
    "# Adversarial Test Loss: 0.1151\n",
    "# Adversarial Accuracy: 96.86%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0430\n",
      "Accuracy: 98.51%\n",
      "Adversarial Test Loss: 4.5166\n",
      "Adversarial Accuracy: 15.01%\n"
     ]
    }
   ],
   "source": [
    "model_reg = new_model()\n",
    "optimizer_reg = optim.Adam(model_reg.parameters(), lr=0.001, weight_decay=5e-4) #with L2 regularization\n",
    "train_model(model_reg, train_loader, criterion, optimizer_reg, epochs=5)\n",
    "test_model(model_reg, test_loader, criterion, optimizer_reg, epsilon=2)\n",
    "\n",
    "# epsilon=2\n",
    "# Adversarial Test Loss: 4.5166\n",
    "# Adversarial Accuracy: 15.01%\n",
    "\n",
    "# epsilon=1\n",
    "# Adversarial Test Loss: 3.4740\n",
    "# Adversarial Accuracy: 19.32%\n",
    "\n",
    "# epsilon=0.5\n",
    "# Epoch 1/5, Loss: 0.1243\n",
    "# Epoch 2/5, Loss: 0.0510\n",
    "# Epoch 3/5, Loss: 0.0421\n",
    "# Epoch 4/5, Loss: 0.0371\n",
    "# Epoch 5/5, Loss: 0.0305\n",
    "# Test Loss: 0.0440\n",
    "# Accuracy: 98.60%\n",
    "# Adversarial Test Loss: 0.9340\n",
    "# Adversarial Accuracy: 70.79%\n",
    "\n",
    "# epsilon=0.1\n",
    "# Adversarial Test Loss: 0.0932\n",
    "# Adversarial Accuracy: 97.01%\n",
    "\n",
    "# epsilon=0.01\n",
    "# Adversarial Test Loss: 0.0470\n",
    "# Adversarial Accuracy: 98.39%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training on an augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We dynamically define images that are adversarial to the current model using fgsm, and train it to work on them\n",
    "def train_model_aug(model, train_loader, criterion, optimizer, epochs=5, epsilon=0.5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss, running_loss_aug = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs.requires_grad = True\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            inputs_aug = (torch.clamp(mean + std*(inputs + epsilon * torch.sign(inputs.grad)), 0, 1) - mean)/ std\n",
    "            outputs_aug = model(inputs_aug)\n",
    "            loss_aug = criterion(outputs_aug, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss_aug.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_aug += loss_aug.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}, Adversarial Loss: {running_loss_aug / len(train_loader):.4f}\")\n",
    "        \n",
    "def train_rand_epsilon(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss, running_loss_aug = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs.requires_grad = True\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            epsilon = np.logspace(-2, 0.8, 100)[np.random.randint(0, 100)] #0.8 emphasizes large epsilons more, as that's a weak point in training\n",
    "            inputs_aug = (torch.clamp(mean + std*(inputs + epsilon * torch.sign(inputs.grad)), 0, 1) - mean)/ std\n",
    "            outputs_aug = model(inputs_aug)\n",
    "            loss_aug = criterion(outputs_aug, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss_aug.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_aug += loss_aug.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}, Adversarial Loss: {running_loss_aug / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0468\n",
      "Accuracy: 98.99%\n",
      "Adversarial Test Loss: 0.8311\n",
      "Adversarial Accuracy: 84.33%\n"
     ]
    }
   ],
   "source": [
    "model_aug = new_model()\n",
    "optimizer_aug = optim.Adam(model_aug.parameters(), lr=0.001)\n",
    "train_model_aug(model_aug, train_loader, criterion, optimizer_aug, epochs=5, epsilon=0.1)\n",
    "test_model(model_aug, test_loader, criterion, optimizer_aug, epsilon=0.5)\n",
    "\n",
    "# epsilon=0.5\n",
    "# Epoch 1/5, Loss: 0.0867, Adversarial Loss: 0.3047\n",
    "# Epoch 2/5, Loss: 0.0292, Adversarial Loss: 0.1468\n",
    "# Epoch 3/5, Loss: 0.0181, Adversarial Loss: 0.1009\n",
    "# Epoch 4/5, Loss: 0.0113, Adversarial Loss: 0.0694\n",
    "# Epoch 5/5, Loss: 0.0076, Adversarial Loss: 0.0601\n",
    "# Test Loss: 0.0258\n",
    "# Accuracy: 99.19%\n",
    "# Adversarial Test Loss: 0.1369\n",
    "# Adversarial Accuracy: 95.92%\n",
    "\n",
    "# epsilon=0.1\n",
    "# Epoch 1/5, Loss: 0.0975, Adversarial Loss: 0.1284\n",
    "# Epoch 2/5, Loss: 0.0326, Adversarial Loss: 0.0520\n",
    "# Epoch 3/5, Loss: 0.0182, Adversarial Loss: 0.0334\n",
    "# Epoch 4/5, Loss: 0.0119, Adversarial Loss: 0.0246\n",
    "# Epoch 5/5, Loss: 0.0080, Adversarial Loss: 0.0189\n",
    "# Test Loss: 0.0468\n",
    "# Accuracy: 98.99%\n",
    "# Adversarial Test Loss: 0.1020\n",
    "# Adversarial Accuracy: 97.80%\n",
    "# testing against epsilon=0.5\n",
    "# Adversarial Test Loss: 0.8311\n",
    "# Adversarial Accuracy: 84.33%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1135, Adversarial Loss: 0.4447\n",
      "Epoch 2/10, Loss: 0.0409, Adversarial Loss: 0.2355\n",
      "Epoch 3/10, Loss: 0.0307, Adversarial Loss: 0.2273\n",
      "Epoch 4/10, Loss: 0.0251, Adversarial Loss: 0.1964\n",
      "Epoch 5/10, Loss: 0.0228, Adversarial Loss: 0.2062\n",
      "Epoch 6/10, Loss: 0.0198, Adversarial Loss: 0.2004\n",
      "Epoch 7/10, Loss: 0.0177, Adversarial Loss: 0.2125\n",
      "Epoch 8/10, Loss: 0.0173, Adversarial Loss: 0.1965\n",
      "Epoch 9/10, Loss: 0.0162, Adversarial Loss: 0.2077\n",
      "Epoch 10/10, Loss: 0.0155, Adversarial Loss: 0.2014\n",
      "\n",
      "epsilon = 3\n",
      "Test Loss: 0.0293\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 0.6358\n",
      "Adversarial Accuracy: 78.93%\n",
      "\n",
      "epsilon = 2\n",
      "Test Loss: 0.0293\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 0.4361\n",
      "Adversarial Accuracy: 86.04%\n",
      "\n",
      "epsilon = 1\n",
      "Test Loss: 0.0293\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 0.4720\n",
      "Adversarial Accuracy: 84.24%\n",
      "\n",
      "epsilon = 0.5\n",
      "Test Loss: 0.0293\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 0.2509\n",
      "Adversarial Accuracy: 91.87%\n",
      "\n",
      "epsilon = 0.1\n",
      "Test Loss: 0.0293\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 0.0613\n",
      "Adversarial Accuracy: 98.16%\n",
      "\n",
      "epsilon = 0.01\n",
      "Test Loss: 0.0293\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 0.0322\n",
      "Adversarial Accuracy: 98.84%\n",
      "\n",
      "epsilon = 0.001\n",
      "Test Loss: 0.0293\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 0.0296\n",
      "Adversarial Accuracy: 98.90%\n"
     ]
    }
   ],
   "source": [
    "model_reg_aug = new_model()\n",
    "optimizer_reg_aug = optim.Adam(model_reg_aug.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "#train_model_aug(model_reg_aug, train_loader, criterion, optimizer_reg_aug, epochs=5, epsilon=3)\n",
    "train_rand_epsilon(model_reg_aug, train_loader, criterion, optimizer_reg_aug, epochs=10)\n",
    "for epsilon in [3, 2, 1, 0.5, 0.1, 0.01, 0.001]:\n",
    "    print(f\"\\nepsilon = {epsilon}\")\n",
    "    test_model(model_reg_aug, test_loader, criterion, optimizer_reg_aug, epsilon=epsilon)\n",
    "\n",
    "# training for epsilon=3 and testing against other epsilons:\n",
    "# Epoch 1/5, Loss: 0.1654, Adversarial Loss: 0.2066\n",
    "# Epoch 2/5, Loss: 0.0526, Adversarial Loss: 0.0651\n",
    "# Epoch 3/5, Loss: 0.0382, Adversarial Loss: 0.0909\n",
    "# Epoch 4/5, Loss: 0.0322, Adversarial Loss: 0.0733\n",
    "# Epoch 5/5, Loss: 0.0255, Adversarial Loss: 0.0793\n",
    "# epsilon = 3\n",
    "# Test Loss: 0.0434\n",
    "# Accuracy: 98.49%\n",
    "# Adversarial Test Loss: 0.0567\n",
    "# Adversarial Accuracy: 97.98%\n",
    "# epsilon = 2\n",
    "# Adversarial Test Loss: 0.1918\n",
    "# Adversarial Accuracy: 94.38%\n",
    "# epsilon = 1\n",
    "# Adversarial Test Loss: 1.6209\n",
    "# Adversarial Accuracy: 50.81%\n",
    "# epsilon = 0.5\n",
    "# Adversarial Test Loss: 1.0487\n",
    "# Adversarial Accuracy: 70.71%\n",
    "# epsilon = 0.1\n",
    "# Adversarial Test Loss: 0.1126\n",
    "# Adversarial Accuracy: 96.66%\n",
    "# epsilon = 0.01\n",
    "# Adversarial Test Loss: 0.0487\n",
    "# Adversarial Accuracy: 98.35%\n",
    "# epsilon = 0.001\n",
    "# Adversarial Test Loss: 0.0440\n",
    "# Adversarial Accuracy: 98.48%\n",
    "\n",
    "# epsilon=1\n",
    "# Epoch 1/5, Loss: 0.1095, Adversarial Loss: 0.5515\n",
    "# Epoch 2/5, Loss: 0.0433, Adversarial Loss: 0.2005\n",
    "# Epoch 3/5, Loss: 0.0345, Adversarial Loss: 0.1508\n",
    "# Epoch 4/5, Loss: 0.0309, Adversarial Loss: 0.1205\n",
    "# Epoch 5/5, Loss: 0.0265, Adversarial Loss: 0.0249\n",
    "# Test Loss: 0.0290\n",
    "# Accuracy: 98.99%\n",
    "# Adversarial Test Loss: 0.0214\n",
    "# Adversarial Accuracy: 99.33%\n",
    "# testing against epsilon=0.5\n",
    "# Adversarial Test Loss: 0.0755\n",
    "# Adversarial Accuracy: 97.51%\n",
    "# against epsilon=0.1\n",
    "# Adversarial Test Loss: 0.0578\n",
    "# Adversarial Accuracy: 98.10%\n",
    "# against epsilon=0.01\n",
    "# Adversarial Test Loss: 0.0313\n",
    "# Adversarial Accuracy: 98.95%\n",
    "# against epsilon=2:\n",
    "# Adversarial Test Loss: 1.3907\n",
    "# Adversarial Accuracy: 65.46%\n",
    "# against epsilon=3\n",
    "# Adversarial Test Loss: 3.4081\n",
    "# Adversarial Accuracy: 40.27%\n",
    "\n",
    "# epsilon=0.5\n",
    "# Epoch 1/5, Loss: 0.0958, Adversarial Loss: 0.2417\n",
    "# Epoch 2/5, Loss: 0.0422, Adversarial Loss: 0.0775\n",
    "# Epoch 3/5, Loss: 0.0336, Adversarial Loss: 0.0624\n",
    "# Epoch 4/5, Loss: 0.0273, Adversarial Loss: 0.0674\n",
    "# Epoch 5/5, Loss: 0.0217, Adversarial Loss: 0.1037\n",
    "# Test Loss: 0.0283\n",
    "# Accuracy: 99.12%\n",
    "# Adversarial Test Loss: 0.1300\n",
    "# Adversarial Accuracy: 96.03%\n",
    "\n",
    "# epsilon=0.1\n",
    "# Epoch 1/5, Loss: 0.1004, Adversarial Loss: 0.1299\n",
    "# Epoch 2/5, Loss: 0.0415, Adversarial Loss: 0.0607\n",
    "# Epoch 3/5, Loss: 0.0321, Adversarial Loss: 0.0487\n",
    "# Epoch 4/5, Loss: 0.0238, Adversarial Loss: 0.0392\n",
    "# Epoch 5/5, Loss: 0.0222, Adversarial Loss: 0.0364\n",
    "# Test Loss: 0.0341\n",
    "# Accuracy: 99.05%\n",
    "# Adversarial Test Loss: 0.0679\n",
    "# Adversarial Accuracy: 97.94%\n",
    "# testing against epsilon=0.5\n",
    "# Adversarial Test Loss: 0.4774\n",
    "# Adversarial Accuracy: 86.31%\n",
    "# testing against epsilon=1\n",
    "# Accuracy: 99.05%\n",
    "# Adversarial Test Loss: 2.2030\n",
    "# Adversarial Accuracy: 39.60%\n",
    "\n",
    "# training on random epsilon, 5 epochs\n",
    "# Epoch 1/5, Loss: 0.1390, Adversarial Loss: 0.3017\n",
    "# Epoch 2/5, Loss: 0.0492, Adversarial Loss: 0.2308\n",
    "# Epoch 3/5, Loss: 0.0383, Adversarial Loss: 0.2195\n",
    "# Epoch 4/5, Loss: 0.0343, Adversarial Loss: 0.2027\n",
    "# Epoch 5/5, Loss: 0.0293, Adversarial Loss: 0.1626\n",
    "# Test Loss: 0.0397\n",
    "# Accuracy: 98.62%\n",
    "# epsilon = 3\n",
    "# Adversarial Test Loss: 0.2398\n",
    "# Adversarial Accuracy: 92.25%\n",
    "# epsilon = 2\n",
    "# Adversarial Test Loss: 0.4328\n",
    "# Adversarial Accuracy: 86.25%\n",
    "# epsilon = 1\n",
    "# Adversarial Test Loss: 0.6709\n",
    "# Adversarial Accuracy: 78.08%\n",
    "# epsilon = 0.5\n",
    "# Adversarial Test Loss: 0.6339\n",
    "# Adversarial Accuracy: 78.81%\n",
    "# epsilon = 0.1\n",
    "# Adversarial Test Loss: 0.1116\n",
    "# Adversarial Accuracy: 96.38%\n",
    "# epsilon = 0.01\n",
    "# Adversarial Test Loss: 0.0451\n",
    "# Adversarial Accuracy: 98.43%\n",
    "# epsilon = 0.001\n",
    "# Adversarial Test Loss: 0.0403\n",
    "# Adversarial Accuracy: 98.60%\n",
    "\n",
    "# with further training, 10 epochs\n",
    "# Epoch 1/5, Loss: 0.0278, Adversarial Loss: 0.1316\n",
    "# Epoch 2/5, Loss: 0.0241, Adversarial Loss: 0.1251\n",
    "# Epoch 3/5, Loss: 0.0265, Adversarial Loss: 0.1395\n",
    "# Epoch 4/5, Loss: 0.0216, Adversarial Loss: 0.0960\n",
    "# Epoch 5/5, Loss: 0.0206, Adversarial Loss: 0.1150\n",
    "# Test Loss: 0.0504\n",
    "# Accuracy: 98.34%\n",
    "# epsilon = 3\n",
    "# Adversarial Test Loss: 0.0751\n",
    "# Adversarial Accuracy: 97.66%\n",
    "# epsilon=2\n",
    "# Adversarial Test Loss: 0.0884\n",
    "# Adversarial Accuracy: 97.29%\n",
    "# epsilon = 1\n",
    "# Adversarial Test Loss: 0.1864\n",
    "# Adversarial Accuracy: 94.28%\n",
    "# epsilon = 0.5\n",
    "# Adversarial Test Loss: 0.4242\n",
    "# Adversarial Accuracy: 86.82%\n",
    "# epsilon = 0.1\n",
    "# Adversarial Test Loss: 0.1524\n",
    "# Adversarial Accuracy: 95.27%\n",
    "# epsilon = 0.01\n",
    "# Adversarial Test Loss: 0.0579\n",
    "# Adversarial Accuracy: 98.09%\n",
    "# epsilon = 0.001\n",
    "# Adversarial Test Loss: 0.0512\n",
    "# Adversarial Accuracy: 98.32%\n",
    "\n",
    "# training on random epsilon with the right distribution\n",
    "# Epoch 1/10, Loss: 0.1023, Adversarial Loss: 0.4621\n",
    "# Epoch 2/10, Loss: 0.0380, Adversarial Loss: 0.2354\n",
    "# Epoch 3/10, Loss: 0.0282, Adversarial Loss: 0.2139\n",
    "# Epoch 4/10, Loss: 0.0224, Adversarial Loss: 0.1783\n",
    "# Epoch 5/10, Loss: 0.0201, Adversarial Loss: 0.1849\n",
    "# Epoch 6/10, Loss: 0.0179, Adversarial Loss: 0.1596\n",
    "# Epoch 7/10, Loss: 0.0174, Adversarial Loss: 0.1691\n",
    "# Epoch 8/10, Loss: 0.0156, Adversarial Loss: 0.1478\n",
    "# Epoch 9/10, Loss: 0.0160, Adversarial Loss: 0.1528\n",
    "# Epoch 10/10, Loss: 0.0141, Adversarial Loss: 0.1320\n",
    "# Test Loss: 0.0316\n",
    "# Accuracy: 99.04%\n",
    "# epsilon = 3\n",
    "# Adversarial Test Loss: 0.6880\n",
    "# Adversarial Accuracy: 78.42%\n",
    "# epsilon = 2\n",
    "# Adversarial Test Loss: 0.3256\n",
    "# Adversarial Accuracy: 89.73%\n",
    "# epsilon = 1\n",
    "# Adversarial Test Loss: 0.3114\n",
    "# Adversarial Accuracy: 90.27%\n",
    "# epsilon = 0.5\n",
    "# Adversarial Test Loss: 0.1965\n",
    "# Adversarial Accuracy: 93.85%\n",
    "# epsilon = 0.1\n",
    "# Adversarial Test Loss: 0.0614\n",
    "# Adversarial Accuracy: 97.99%\n",
    "# epsilon = 0.01\n",
    "# Adversarial Test Loss: 0.0345\n",
    "# Adversarial Accuracy: 98.97%\n",
    "# epsilon = 0.001\n",
    "# Adversarial Test Loss: 0.0319\n",
    "# Adversarial Accuracy: 99.04%\n",
    "\n",
    "# training with high-biased random epsilon:\n",
    "# Epoch 1/10, Loss: 0.1135, Adversarial Loss: 0.4447\n",
    "# Epoch 2/10, Loss: 0.0409, Adversarial Loss: 0.2355\n",
    "# Epoch 3/10, Loss: 0.0307, Adversarial Loss: 0.2273\n",
    "# Epoch 4/10, Loss: 0.0251, Adversarial Loss: 0.1964\n",
    "# Epoch 5/10, Loss: 0.0228, Adversarial Loss: 0.2062\n",
    "# Epoch 6/10, Loss: 0.0198, Adversarial Loss: 0.2004\n",
    "# Epoch 7/10, Loss: 0.0177, Adversarial Loss: 0.2125\n",
    "# Epoch 8/10, Loss: 0.0173, Adversarial Loss: 0.1965\n",
    "# Epoch 9/10, Loss: 0.0162, Adversarial Loss: 0.2077\n",
    "# Epoch 10/10, Loss: 0.0155, Adversarial Loss: 0.2014\n",
    "# Test Loss: 0.0293\n",
    "# Accuracy: 98.90%\n",
    "# epsilon = 3\n",
    "# Adversarial Test Loss: 0.6358\n",
    "# Adversarial Accuracy: 78.93%\n",
    "# epsilon = 2\n",
    "# Adversarial Test Loss: 0.4361\n",
    "# Adversarial Accuracy: 86.04%\n",
    "# epsilon = 1\n",
    "# Adversarial Test Loss: 0.4720\n",
    "# Adversarial Accuracy: 84.24%\n",
    "# epsilon = 0.5\n",
    "# Adversarial Test Loss: 0.2509\n",
    "# Adversarial Accuracy: 91.87%\n",
    "# epsilon = 0.1\n",
    "# Adversarial Test Loss: 0.0613\n",
    "# Adversarial Accuracy: 98.16%\n",
    "# epsilon = 0.01\n",
    "# Adversarial Test Loss: 0.0322\n",
    "# Adversarial Accuracy: 98.84%\n",
    "# epsilon = 0.001\n",
    "# Adversarial Test Loss: 0.0296\n",
    "# Adversarial Accuracy: 98.90%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Typical training is very weak to adversarial attacks, and quickly mislabels adversarial attacks.\n",
    "\n",
    "Regularization hardly improves robustness.\n",
    "\n",
    "Training on very adversarial data creates incredibly robust models to that attack. However training against a constant attack strength does not grant robustness against all strengths. This is achieved fairly well by training against attacks of random, variable strength. With this training, we achieve nearly 95% accuracy on adversarial data while maintaining 99% accuracy on real data. We remain slightly weak to super adversarial attacks. However, very strong attacks are trivial to recognize for an appropriately trained network (see null-labeling.ipynb), and we have over 98% accuracy on weak attacks which can't be recognized with absolute certainty.\n",
    "\n",
    "Overall, training against fgsm appears very simple and a model modified to be robust against it will resist these attacks very well. It is noteworthy that the cost in accuracy on real data is surprisingly nonexistent: this might be due to the model we took being quite large, so it didn't need all the neurons to recognize MNIST anyway. What would be more interesting is to work with multi-step fgsm, increasing the number of steps needed to lead to misclassification. We did not explore this for computational purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
