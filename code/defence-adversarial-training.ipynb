{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Implementation of \"adversarial training\" as a defence mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training on augmented dataset\n",
    "### 1.1 Training on regular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We take the MNIST dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) \n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform) #60k images\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform) #10k images\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 1x28x28 -> 32x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 32x28x28 -> 64x28x28\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2), # 64x28x28 -> 64x14x14\n",
    "        nn.Flatten(),  # 64x14x14 -> 12544\n",
    "        nn.Linear(64 * 14 * 14, 128),  # 12544 -> 128\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)  # 128 -> 10\n",
    "    )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def test_model(model, test_loader, criterion, optimizer, epsilon=0.1):\n",
    "    model.eval()\n",
    "    correct, total, test_loss = 0, 0, 0\n",
    "    correct_aug, test_loss_aug = 0, 0\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs.requires_grad = True\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss.backward() #gets gradient \n",
    "        inputs_aug = torch.clamp(inputs + epsilon * torch.sign(inputs.grad), 0, 1)\n",
    "        outputs_aug = model(inputs_aug)\n",
    "        loss_aug = criterion(outputs_aug, labels)\n",
    "        test_loss_aug += loss_aug.item()\n",
    "\n",
    "        _, predicted_aug = torch.max(outputs_aug, 1)\n",
    "        correct_aug += (predicted_aug == labels).sum().item()\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "    print(f\"Adversarial Test Loss: {test_loss_aug / len(test_loader):.4f}\")\n",
    "    print(f\"Adversarial Accuracy: {100 * correct_aug / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1341\n",
      "Epoch 2/10, Loss: 0.0412\n",
      "Epoch 3/10, Loss: 0.0255\n",
      "Epoch 4/10, Loss: 0.0165\n",
      "Epoch 5/10, Loss: 0.0145\n",
      "Epoch 6/10, Loss: 0.0101\n",
      "Epoch 7/10, Loss: 0.0094\n",
      "Epoch 8/10, Loss: 0.0064\n",
      "Epoch 9/10, Loss: 0.0050\n",
      "Epoch 10/10, Loss: 0.0066\n",
      "Test Loss: 0.0480\n",
      "Accuracy: 98.93%\n",
      "Adversarial Test Loss: 0.4252\n",
      "Adversarial Accuracy: 86.55%\n"
     ]
    }
   ],
   "source": [
    "model = new_model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5) #2 epochs decent, 10 would overfit, 5 is good\n",
    "test_model(model, test_loader, criterion, optimizer)\n",
    "\n",
    "# Unregularized training ends up overfitting\n",
    "# Epoch 1/10, Loss: 0.1341\n",
    "# Epoch 2/10, Loss: 0.0412\n",
    "# Epoch 3/10, Loss: 0.0255\n",
    "# Epoch 4/10, Loss: 0.0165\n",
    "# Epoch 5/10, Loss: 0.0145\n",
    "# Epoch 6/10, Loss: 0.0101\n",
    "# Epoch 7/10, Loss: 0.0094\n",
    "# Epoch 8/10, Loss: 0.0064\n",
    "# Epoch 9/10, Loss: 0.0050\n",
    "# Epoch 10/10, Loss: 0.0066\n",
    "# Test Loss: 0.0480\n",
    "# Accuracy: 98.93%\n",
    "# Adversarial Test Loss: 0.4252\n",
    "# Adversarial Accuracy: 86.55%\n",
    "\n",
    "# Here's a shorter training, that avoids overfitting, but is not robust\n",
    "# Epoch 1/2, Loss: 0.1335\n",
    "# Epoch 2/2, Loss: 0.0396\n",
    "# Test Loss: 0.0373\n",
    "# Accuracy: 98.86%\n",
    "# Adversarial Test Loss: 0.6968\n",
    "# Adversarial Accuracy: 74.06%\n",
    "# We see it's very weak to fgsm\n",
    "\n",
    "# Medium length training is more robust and performs pretty well\n",
    "# Epoch 1/5, Loss: 0.1354\n",
    "# Epoch 2/5, Loss: 0.0408\n",
    "# Epoch 3/5, Loss: 0.0255\n",
    "# Epoch 4/5, Loss: 0.0172\n",
    "# Epoch 5/5, Loss: 0.0130\n",
    "# Test Loss: 0.0416\n",
    "# Accuracy: 98.83%\n",
    "# Adversarial Test Loss: 0.3076\n",
    "# Adversarial Accuracy: 89.62%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.1327\n",
      "Epoch 2/5, Loss: 0.0427\n",
      "Epoch 3/5, Loss: 0.0309\n",
      "Epoch 4/5, Loss: 0.0230\n",
      "Epoch 5/5, Loss: 0.0199\n",
      "Test Loss: 0.0387\n",
      "Accuracy: 98.90%\n",
      "Adversarial Test Loss: 0.2664\n",
      "Adversarial Accuracy: 92.96%\n"
     ]
    }
   ],
   "source": [
    "model_reg = new_model()\n",
    "optimizer_reg = optim.Adam(model_reg.parameters(), lr=0.001, weight_decay=5e-4) #with L2 regularization\n",
    "train_model(model_reg, train_loader, criterion, optimizer_reg, epochs=5)\n",
    "test_model(model_reg, test_loader, criterion, optimizer_reg)\n",
    "\n",
    "# Ridge regularization increases robustness, but the accuracy drop is still fairly significant\n",
    "# Epoch 1/5, Loss: 0.1327\n",
    "# Epoch 2/5, Loss: 0.0427\n",
    "# Epoch 3/5, Loss: 0.0309\n",
    "# Epoch 4/5, Loss: 0.0230\n",
    "# Epoch 5/5, Loss: 0.0199\n",
    "# Test Loss: 0.0387\n",
    "# Accuracy: 98.90%\n",
    "# Adversarial Test Loss: 0.2664\n",
    "# Adversarial Accuracy: 92.96%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training on an augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We dynamically define images that are adversarial to the current model using fgsm, and train it to work on them\n",
    "def train_model_aug(model, train_loader, criterion, optimizer, epochs=5, epsilon=0.1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss, running_loss_aug = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs.requires_grad = True\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            inputs_aug = torch.clamp(inputs + epsilon * torch.sign(inputs.grad), 0, 1)\n",
    "            outputs_aug = model(inputs_aug)\n",
    "            loss_aug = criterion(outputs_aug, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss_aug.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_aug += loss_aug.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}, Adversarial Loss: {running_loss_aug / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.1182, Adversarial Loss: 0.1263\n",
      "Epoch 2/5, Loss: 0.0392, Adversarial Loss: 0.0387\n",
      "Epoch 3/5, Loss: 0.0225, Adversarial Loss: 0.0209\n",
      "Epoch 4/5, Loss: 0.0161, Adversarial Loss: 0.0136\n",
      "Epoch 5/5, Loss: 0.0115, Adversarial Loss: 0.0091\n",
      "Test Loss: 0.0620\n",
      "Accuracy: 98.81%\n",
      "Adversarial Test Loss: 0.1727\n",
      "Adversarial Accuracy: 96.10%\n"
     ]
    }
   ],
   "source": [
    "model_aug = new_model()\n",
    "optimizer_aug = optim.Adam(model_aug.parameters(), lr=0.001)\n",
    "train_model_aug(model_aug, train_loader, criterion, optimizer_aug, epochs=5)\n",
    "test_model(model_aug, test_loader, criterion, optimizer_aug)\n",
    "\n",
    "\n",
    "# It performs no worse on the real data, and its adversarial accuracy is now a lot stronger (and we still overfit, but that's somewhat irrelevant)\n",
    "# Epoch 1/5, Loss: 0.1182, Adversarial Loss: 0.1263\n",
    "# Epoch 2/5, Loss: 0.0392, Adversarial Loss: 0.0387\n",
    "# Epoch 3/5, Loss: 0.0225, Adversarial Loss: 0.0209\n",
    "# Epoch 4/5, Loss: 0.0161, Adversarial Loss: 0.0136\n",
    "# Epoch 5/5, Loss: 0.0115, Adversarial Loss: 0.0091\n",
    "# Test Loss: 0.0620\n",
    "# Accuracy: 98.81%\n",
    "# Adversarial Test Loss: 0.1727\n",
    "# Adversarial Accuracy: 96.10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.1311, Adversarial Loss: 0.1420\n",
      "Epoch 2/5, Loss: 0.0429, Adversarial Loss: 0.0440\n",
      "Epoch 3/5, Loss: 0.0273, Adversarial Loss: 0.0271\n",
      "Epoch 4/5, Loss: 0.0214, Adversarial Loss: 0.0200\n",
      "Epoch 5/5, Loss: 0.0182, Adversarial Loss: 0.0162\n",
      "Test Loss: 0.0618\n",
      "Accuracy: 98.44%\n",
      "Adversarial Test Loss: 0.1739\n",
      "Adversarial Accuracy: 95.28%\n"
     ]
    }
   ],
   "source": [
    "model_reg_aug = new_model()\n",
    "optimizer_reg_aug = optim.Adam(model_reg_aug.parameters(), lr=0.001, weight_decay=3e-5)\n",
    "train_model_aug(model_reg_aug, train_loader, criterion, optimizer_reg_aug, epochs=5)\n",
    "test_model(model_reg_aug, test_loader, criterion, optimizer_reg_aug)\n",
    "\n",
    "# With weight_decay=5e-4, the regularized model is less robust because we forced it to be too simple\n",
    "# Epoch 1/5, Loss: 0.1341, Adversarial Loss: 0.1460\n",
    "# Epoch 2/5, Loss: 0.0608, Adversarial Loss: 0.0635\n",
    "# Epoch 3/5, Loss: 0.0486, Adversarial Loss: 0.0518\n",
    "# Epoch 4/5, Loss: 0.0403, Adversarial Loss: 0.0439\n",
    "# Epoch 5/5, Loss: 0.0371, Adversarial Loss: 0.0406\n",
    "# Test Loss: 0.0394\n",
    "# Accuracy: 98.68%\n",
    "# Adversarial Test Loss: 0.1396\n",
    "# Adversarial Accuracy: 95.24%\n",
    "\n",
    "# weight_decay=1-4\n",
    "# Epoch 1/5, Loss: 0.1241, Adversarial Loss: 0.1352\n",
    "# Epoch 2/5, Loss: 0.0472, Adversarial Loss: 0.0487\n",
    "# Epoch 3/5, Loss: 0.0340, Adversarial Loss: 0.0347\n",
    "# Epoch 4/5, Loss: 0.0289, Adversarial Loss: 0.0283\n",
    "# Epoch 5/5, Loss: 0.0234, Adversarial Loss: 0.0233\n",
    "# Test Loss: 0.0480\n",
    "# Accuracy: 98.65%\n",
    "# Adversarial Test Loss: 0.1340\n",
    "# Adversarial Accuracy: 95.99%\n",
    "\n",
    "# weight_decay=3e-5\n",
    "# Epoch 1/5, Loss: 0.1311, Adversarial Loss: 0.1420\n",
    "# Epoch 2/5, Loss: 0.0429, Adversarial Loss: 0.0440\n",
    "# Epoch 3/5, Loss: 0.0273, Adversarial Loss: 0.0271\n",
    "# Epoch 4/5, Loss: 0.0214, Adversarial Loss: 0.0200\n",
    "# Epoch 5/5, Loss: 0.0182, Adversarial Loss: 0.0162\n",
    "# Test Loss: 0.0618\n",
    "# Accuracy: 98.44%\n",
    "# Adversarial Test Loss: 0.1739\n",
    "# Adversarial Accuracy: 95.28%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Typical training is very weak to adversarial attacks, and quickly mislabels adversarial attacks\n",
    "Regularization improves robustness significantly.\n",
    "However, training directly on adversarial images increases robustness very significantly, while leading to no noticeable accuracy loss on real data\n",
    "Further testing against larger epsilons (i.e. more adversarial images) is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training with modified loss function\n",
    "We follow the approach described by Goodfellow et al. and change the loss function to:\n",
    "$$\\tilde{J}_\\theta(x, y) = \\alpha J_\\theta(x, y) + (1-\\alpha)J_\\theta\\left(x+\\varepsilon\\text{ sign}\\left(\\nabla_x J_\\theta(x, y)\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
